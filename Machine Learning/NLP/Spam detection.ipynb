{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_table('SMSSpamCollection', sep = \"\\t\", names = ['SMS Type', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMS Type</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SMS Type  \\\n",
       "0      ham   \n",
       "1      ham   \n",
       "2     spam   \n",
       "3      ham   \n",
       "4      ham   \n",
       "\n",
       "                                                                                                                                                          Text  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                            U dun say so early hor... U c already then say...  \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Text Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    \"\"\"\n",
    "    The cleaning_text function takes a text and returns a clean text with no punctuation\n",
    "    and composed only of the roots of every word that is not a stopword in the English language\n",
    "    \n",
    "    @param text is the text that you want the cleaning_function to be applied to\n",
    "    \"\"\"\n",
    "    import re \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "    # creating an object from the class PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # remove any punctuation from the text and replace with space\n",
    "    clean_text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "\n",
    "    # change all letters to lowercase\n",
    "    clean_text = clean_text.lower()\n",
    "\n",
    "    # create a list of words from the text message\n",
    "    clean_text = clean_text.split()\n",
    "\n",
    "    # check if each word is a stopword and if not keep only its root\n",
    "    clean_text = [ps.stem(word) for word in clean_text if word not in set(stopwords.words('english'))]\n",
    "\n",
    "    # convert that list of words into a string with space as the delimeter, in order to create a bag of words in the future\n",
    "    clean_text = \" \".join(clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the number of texts in the dataset \n",
    "num_texts = dataset.shape[0]\n",
    "num_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [cleaning_text(each_text) for each_text in dataset['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           go jurong point crazi avail bugi n great world la e buffet cine got amor wat\n",
       "1                                                                                  ok lar joke wif u oni\n",
       "2    free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli\n",
       "3                                                                    u dun say earli hor u c alreadi say\n",
       "4                                                                   nah think goe usf live around though\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring the corpus\n",
    "expl_corpus = pd.Series(corpus, name = \"Text\")\n",
    "expl_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a bag of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# creating the bag of words matrix\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 6296)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the dimension of our bag of words\n",
    "# we have 5572 text messages but these text messages are composed of 6296 differents words.\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The matrix below shows that there are lot of words that almost never appear in most of the reviews.\n",
    "# and this will create a sparse matrix, which is a problem for machine learning algorithm\n",
    "# we will try to decrease the sparsity of our matrix X by remove those words.\n",
    "# We can have an idea of those words by looking at the expl_corpus above. words like st, usf, and so on need to be removed\n",
    "# additional expertise is necessary to do a good nlp problem\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we go back and change the max_features to a lower number in the CountVectorizer object during object instantiation.\n",
    "# This will get rid of words that appear rarely in text.  \n",
    "cv = CountVectorizer(max_features = 6000) # our bag of words will have only the 6000 most frequent words as columns. from 6296\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dependent variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = dataset['SMS Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ham\n",
       "1        ham\n",
       "2       spam\n",
       "3        ham\n",
       "4        ham\n",
       "5       spam\n",
       "6        ham\n",
       "7        ham\n",
       "8       spam\n",
       "9       spam\n",
       "10       ham\n",
       "11      spam\n",
       "12      spam\n",
       "13       ham\n",
       "14       ham\n",
       "15      spam\n",
       "16       ham\n",
       "17       ham\n",
       "18       ham\n",
       "19      spam\n",
       "20       ham\n",
       "21       ham\n",
       "22       ham\n",
       "23       ham\n",
       "24       ham\n",
       "25       ham\n",
       "26       ham\n",
       "27       ham\n",
       "28       ham\n",
       "29       ham\n",
       "        ... \n",
       "5542     ham\n",
       "5543     ham\n",
       "5544     ham\n",
       "5545     ham\n",
       "5546     ham\n",
       "5547    spam\n",
       "5548     ham\n",
       "5549     ham\n",
       "5550     ham\n",
       "5551     ham\n",
       "5552     ham\n",
       "5553     ham\n",
       "5554     ham\n",
       "5555     ham\n",
       "5556     ham\n",
       "5557     ham\n",
       "5558     ham\n",
       "5559     ham\n",
       "5560     ham\n",
       "5561     ham\n",
       "5562     ham\n",
       "5563     ham\n",
       "5564     ham\n",
       "5565     ham\n",
       "5566    spam\n",
       "5567    spam\n",
       "5568     ham\n",
       "5569     ham\n",
       "5570     ham\n",
       "5571     ham\n",
       "Name: SMS Type, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the dataset into training set and testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting Naive Bayes to the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # perform better than the previous (GaussianNB)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting the test result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making the confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f4932c2828>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEFCAYAAAACFke6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwP8BDawf8ZOHOy8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f49329db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_names = ['Ham','Spam']\n",
    "sns.heatmap(cm, annot = True, fmt = \"d\",# fmt can be .2f\n",
    "            square = False, linewidths = 1, linecolor = 'black',\n",
    "           cbar = False, xticklabels = target_names, yticklabels = target_names) \n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classification metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this model is: 0.980\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy score for this model is: {:.3f}\".format(sklearn.metrics.accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.99      0.99       955\n",
      "       spam       0.92      0.95      0.93       160\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
